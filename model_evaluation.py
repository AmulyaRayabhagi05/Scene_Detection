# -*- coding: utf-8 -*-
"""Model Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRoPGcyypp8Gk2cBlQjxsuM6ZYjYrUrv
"""

import numpy as np
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import warnings
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score, roc_curve, auc
import seaborn as sns
from sklearn.metrics import classification_report
import time
import json
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize

os.environ ['KAGGLE_USERNAME'] = ""
os.environ ['KAGGLE_KEY'] = ""
!kaggle datasets download -d puneet6060/intel-image-classification
!unzip intel-image-classification.zip -d data/
!unzip data/seg_train.zip -d data/

# ConfigS (change these for log file and various optimizations. Image size can affect RAM so don't do too high)
DATA_DIR = "data/seg_train/seg_train"
TEST_SIZE = 0.2
IMG_SIZE = (64, 64)
EPOCHS = 60
LEARNING_RATE = 0.0043
NUM_SAMPLES = 10000  # Adjust this number to use more or less data for a quick run

def load_data(data_dir, img_size, num_samples):
    """
    Loads images and labels from the specified directory.
    Resizes images and stores them as numpy arrays.
    """
    images = []
    labels = []
    class_names = sorted(os.listdir(data_dir))
    class_to_idx = {cls: i for i, cls in enumerate(class_names)}

    print("Loading data...")
    count = 0
    for cls in class_names:
        class_path = os.path.join(data_dir, cls)
        for img_name in os.listdir(class_path):
            if count >= num_samples:
                break
            img_path = os.path.join(class_path, img_name)
            try:
                img = Image.open(img_path).resize(img_size)
                img = np.array(img).astype('float32') / 255.0  # Normalize to [0, 1]
                images.append(img)
                labels.append(class_to_idx[cls])
                count += 1
            except Exception as e:
                print(f"Error loading image {img_path}: {e}")
        if count >= num_samples:
            break

    # Convert lists to numpy arrays
    X = np.array(images)
    y = np.array(labels)

    # Convert to one-hot encoding
    num_classes = len(class_names)
    y_one_hot = np.zeros((y.size, num_classes))
    y_one_hot[np.arange(y.size), y] = 1

    print(f"Data loaded. Total samples: {len(X)}")
    print(f"Image shape: {X.shape[1:]}")
    print(f"Number of classes: {num_classes}")

    return X, y_one_hot, y, class_names

class ConvLayer:
    def __init__(self, num_filters, filter_size, input_shape):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.input_shape = input_shape
        self.in_channels = input_shape[2]
        # He initialization with correct fan-in
        scale = np.sqrt(2.0 / (filter_size * filter_size * self.in_channels))
        self.filters = np.random.randn(filter_size, filter_size, self.in_channels, num_filters) * scale
        self.biases = np.zeros(num_filters)
        self.last_input = None

    def forward(self, input_data):
        self.last_input = input_data
        batch_size, in_height, in_width, _ = input_data.shape
        out_height = in_height - self.filter_size + 1
        out_width = in_width - self.filter_size + 1

        output = np.zeros((batch_size, out_height, out_width, self.num_filters))

        # convolution using im2col for each position
        for h in range(out_height):
            h_end = h + self.filter_size
            for w in range(out_width):
                w_end = w + self.filter_size
                # Extract patch for all images in batch
                patch = input_data[:, h:h_end, w:w_end, :]  # shape: [batch, filter_size, filter_size, in_channels]
                # Vectorized computation across filters
                output[:, h, w, :] = np.tensordot(patch, self.filters, axes=([1,2,3], [0,1,2])) + self.biases

        return output

    def backward(self, d_output):
        batch_size, out_h, out_w, num_filters = d_output.shape
        d_filters = np.zeros_like(self.filters)
        d_biases = np.sum(d_output, axis=(0,1,2))
        d_input = np.zeros_like(self.last_input)

        # Compute gradients
        for h in range(out_h):
            for w in range(out_w):
                h_start, h_end = h, h + self.filter_size
                w_start, w_end = w, w + self.filter_size

                # Get the input patch that produced this output
                patch = self.last_input[:, h_start:h_end, w_start:w_end, :]

                # Gradient for filters (accumulate across batch)
                d_filters += np.tensordot(patch, d_output[:, h, w, :], axes=([0], [0]))

                # Gradient for input
                d_input[:, h_start:h_end, w_start:w_end, :] += np.tensordot(
                    d_output[:, h, w, :],
                    self.filters,
                    axes=([1], [3])
                )

        return d_input, d_filters, d_biases

class ReLULayer:
    def __init__(self):
        self.last_input = None

    def forward(self, input_data):
        self.last_input = input_data
        return np.maximum(0, input_data)

    def backward(self, d_output):
        d_input = d_output.copy()
        d_input[self.last_input <= 0] = 0
        return d_input

class MaxPoolingLayer:
    def __init__(self, pool_size, stride):
        self.pool_size = pool_size
        self.stride = stride
        self.last_input = None
        self.mask = None

    def forward(self, input_data):
        self.last_input = input_data
        batch_size, in_h, in_w, in_channels = input_data.shape
        out_h = (in_h - self.pool_size) // self.stride + 1
        out_w = (in_w - self.pool_size) // self.stride + 1

        # Pre-allocate output
        output = np.zeros((batch_size, out_h, out_w, in_channels))

        # Vectorized max pooling using reshape and max
        for h in range(out_h):
            h_start = h * self.stride
            h_end = h_start + self.pool_size
            for w in range(out_w):
                w_start = w * self.stride
                w_end = w_start + self.pool_size

                # Extract the current window
                window = input_data[:, h_start:h_end, w_start:w_end, :]

                # Reshape to flatten spatial dimensions and find max
                window_reshaped = window.reshape(batch_size, -1, in_channels)
                output[:, h, w, :] = np.max(window_reshaped, axis=1)

        # Create mask for backward pass (done in backward to save memory)
        self.mask = None  # We'll recreate it during backward pass

        return output

    def backward(self, d_output):
        batch_size, out_h, out_w, in_channels = d_output.shape
        d_input = np.zeros_like(self.last_input)

        for h in range(out_h):
            h_start = h * self.stride
            h_end = h_start + self.pool_size
            for w in range(out_w):
                w_start = w * self.stride
                w_end = w_start + self.pool_size

                window = self.last_input[:, h_start:h_end, w_start:w_end, :]
                window_reshaped = window.reshape(batch_size, -1, in_channels)
                max_vals = np.max(window_reshaped, axis=1)

                # Create mask for this window
                mask_window = (window == max_vals[:, None, None, :])

                # Distribute gradients
                d_input[:, h_start:h_end, w_start:w_end, :] += (
                    d_output[:, h, w, :][:, None, None, :] * mask_window
                )

        return d_input
        return d_input

class FlattenLayer:
    def __init__(self):
        self.last_input_shape = None

    def forward(self, input_data):
        self.last_input_shape = input_data.shape
        return input_data.reshape(input_data.shape[0], -1)

    def backward(self, d_output):
        return d_output.reshape(self.last_input_shape)

class FCLayer:
    def __init__(self, input_size, output_size):
        self.weights = np.random.randn(input_size, output_size) * 0.01
        self.biases = np.zeros(output_size)
        self.last_input = None

    def forward(self, input_data):
        self.last_input = input_data
        return np.dot(input_data, self.weights) + self.biases

    def backward(self, d_output):
        d_weights = np.dot(self.last_input.T, d_output)
        d_biases = np.sum(d_output, axis=0)
        d_input = np.dot(d_output, self.weights.T)
        return d_input, d_weights, d_biases

class CNN:
    def __init__(self, input_shape, num_classes):
        self.layers = [
            ConvLayer(num_filters=8, filter_size=3, input_shape=input_shape),
            ReLULayer(),
            MaxPoolingLayer(pool_size=2, stride=2),
            FlattenLayer(),
            FCLayer(input_size=31 * 31 * 8, output_size=num_classes)
        ]

    def forward(self, input_data):
        out = input_data
        for layer in self.layers:
            out = layer.forward(out)
        return out

    def backward(self, d_output):
        d_input = d_output
        d_weights_fc, d_biases_fc = None, None
        d_filters_conv, d_biases_conv = None, None

        # Backprop through FC Layer
        d_input, d_weights_fc, d_biases_fc = self.layers[-1].backward(d_input)

        # Backprop through Flatten
        d_input = self.layers[-2].backward(d_input)

        # Backprop through Max Pool
        d_input = self.layers[-3].backward(d_input)

        # Backprop through ReLU
        d_input = self.layers[-4].backward(d_input)

        # Backprop through Conv
        _, d_filters_conv, d_biases_conv = self.layers[-5].backward(d_input)

        return d_weights_fc, d_biases_fc, d_filters_conv, d_biases_conv

    def update_params(self, d_weights_fc, d_biases_fc, d_filters_conv, d_biases_conv, learning_rate):
        self.layers[-1].weights -= learning_rate * d_weights_fc
        self.layers[-1].biases -= learning_rate * d_biases_fc
        self.layers[-5].filters -= learning_rate * d_filters_conv
        self.layers[-5].biases -= learning_rate * d_biases_conv

# Loss function & optimizer

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def cross_entropy_loss(predictions, targets):
    batch_size = predictions.shape[0]
    predictions_clipped = np.clip(predictions, 1e-12, 1 - 1e-12)
    loss = -np.sum(targets * np.log(predictions_clipped)) / batch_size
    return loss

def backward_loss(predictions, targets):
    batch_size = predictions.shape[0]
    d_input = (predictions - targets) / batch_size
    return d_input

# Main training loop

def train_model(model, X_train, y_train, epochs, learning_rate):
    print("\nStarting training...")
    for epoch in range(epochs):
        # Shuffle data for each epoch
        permutation = np.random.permutation(X_train.shape[0])
        X_train_shuffled = X_train[permutation]
        y_train_shuffled = y_train[permutation]

        x_batch = X_train_shuffled
        y_batch = y_train_shuffled

        # Forward pass
        predictions_raw = model.forward(x_batch)
        predictions = softmax(predictions_raw)

        # Calculate loss
        loss = cross_entropy_loss(predictions, y_batch)

        # Backward pass
        d_loss = backward_loss(predictions, y_batch)
        d_weights_fc, d_biases_fc, d_filters_conv, d_biases_conv = model.backward(d_loss)

        # Update parameters
        model.update_params(d_weights_fc, d_biases_fc, d_filters_conv, d_biases_conv, learning_rate)

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")
    print("Training finished.")

def evaluate_model(model, X_test, y_test, class_names, history=None, batch_size=256, outdir="artifacts"):
    os.makedirs(outdir, exist_ok=True)
    print("\nStart evaluation...")

    n = X_test.shape[0]
    num_classes = len(class_names)

    # y_true as class indices
    if y_test.ndim == 2 and y_test.shape[1] == num_classes:
        y_true = np.argmax(y_test, axis=1)
    else:
        y_true = y_test.astype(int).ravel()


    y_pred = np.empty(n, dtype=np.int32)
    y_score = np.zeros((n, num_classes), dtype=np.float32)  # for ROC

    for start in range(0, n, batch_size):
        end = min(start + batch_size, n)
        logits = model.forward(X_test[start:end])
        probs = softmax(logits)
        y_score[start:end] = probs
        y_pred[start:end] = np.argmax(probs, axis=1)

    # Metrics
    acc = accuracy_score(y_true, y_pred)
    print(f"Model accuracy on test set: {acc:.4f}")

    # Classification report
    labels = list(range(num_classes))
    report_dict = classification_report(
        y_true, y_pred,
        labels=labels,
        target_names=class_names,
        output_dict=True,
        zero_division=0
    )
    report_df = pd.DataFrame(report_dict).transpose()
    report_csv_path = os.path.join(outdir, "classification_report.csv")
    report_txt_path = os.path.join(outdir, "classification_report.txt")
    report_df.to_csv(report_csv_path)
    with open(report_txt_path, "w") as f:
        f.write(classification_report(
            y_true, y_pred,
            labels=labels,
            target_names=class_names,
            zero_division=0
        ))

    # per-class arrays for compact table + bars
    prec, rec, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, zero_division=0
    )
    metrics_df = pd.DataFrame({
        "class": class_names,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "support": support
    })
    metrics_csv_path = os.path.join(outdir, "per_class_metrics.csv")
    metrics_df.to_csv(metrics_csv_path, index=False)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    plt.figure(figsize=(6, 5))
    plt.imshow(cm, interpolation="nearest")
    plt.title("Confusion Matrix")
    plt.xticks(ticks=np.arange(num_classes), labels=class_names, rotation=45, ha="right")
    plt.yticks(ticks=np.arange(num_classes), labels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    for i in range(num_classes):
        for j in range(num_classes):
            plt.text(j, i, str(cm[i, j]), ha="center", va="center", fontsize=8)
    plt.tight_layout()
    cm_path = os.path.join(outdir, "confusion_matrix.png")
    plt.savefig(cm_path, dpi=150, bbox_inches="tight")
    plt.close()

    # Precision/recall/F1
    plt.figure(figsize=(7.5, 4.5))
    x = np.arange(num_classes)
    width = 0.25
    plt.bar(x - width, prec, width, label="Precision")
    plt.bar(x,        rec,  width, label="Recall")
    plt.bar(x + width, f1,  width, label="F1")
    plt.xticks(x, class_names, rotation=45, ha="right")
    plt.ylabel("Score")
    plt.title("Per-class metrics")
    plt.legend()
    plt.tight_layout()
    bars_path = os.path.join(outdir, "per_class_bars.png")
    plt.savefig(bars_path, dpi=150, bbox_inches="tight")
    plt.close()

    #  ROC Curve (One-vs-Rest, multi-class)
    y_true_bin = label_binarize(y_true, classes=labels)

    plt.figure(figsize=(6, 5))
    for i in range(num_classes):
        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1.5, label=f"{class_names[i]} (AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], 'k--', lw=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Multi-class ROC Curve")
    plt.legend(loc="lower right", fontsize=8)
    plt.tight_layout()
    roc_path = os.path.join(outdir, "roc_curve.png")
    plt.savefig(roc_path, dpi=150, bbox_inches="tight")
    plt.close()

    try:
        from IPython.display import display
        print("\n=== Per-class metrics (head) ===")
        display(metrics_df[["class", "precision", "recall", "f1", "support"]].round(3))
    except Exception:
        pass

    print("\nArtifacts saved:")
    print(f" - {report_csv_path}")
    print(f" - {report_txt_path}")
    print(f" - {metrics_csv_path}")
    print(f" - {cm_path}")
    print(f" - {bars_path}")
    print(f" - {roc_path}")

if __name__ == "__main__":
    # Load and split data
    X, y_one_hot, y_labels, class_names = load_data(DATA_DIR, IMG_SIZE, NUM_SAMPLES)
    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=TEST_SIZE, random_state=42)

    # Initialize and train the model
    input_shape = X_train.shape[1:]
    num_classes = len(class_names)
    cnn_model = CNN(input_shape, num_classes)

    train_model(cnn_model, X_train, y_train, EPOCHS, LEARNING_RATE)

    # Evaluate the model
    evaluate_model(cnn_model, X_test, y_test, class_names)

from IPython.display import Image, display

display(Image(filename="artifacts/confusion_matrix.png"))
display(Image(filename="artifacts/per_class_bars.png"))
display(Image(filename="artifacts/roc_curve.png"))
